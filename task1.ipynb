{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1\n",
    "riconoscimento review positiva o negativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import libraries.preprocessing_utils as prep_utils\n",
    "from libraries.dataset import Dataset\n",
    "\n",
    "# import libraries.utils as utils\n",
    "import constants as const\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data retrieving + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./data/balanced_review_sentiment_train.csv...\n",
      "File loaded in 0.2 minutes\n",
      "Reading ./data/balanced_review_sentiment_val.csv...\n",
      "File loaded in 0.0 minutes\n",
      "Reading ./data/balanced_review_sentiment_test.csv...\n",
      "File loaded in 0.0 minutes\n"
     ]
    }
   ],
   "source": [
    "review_data = Dataset('review', 'sentiment')\n",
    "review_data.split(['text'], 'sentiment', n_samples=500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text -> list[str]\n",
    "# fit tokenizer and tokenize\n",
    "tokenizer = prep_utils.get_tokenizer(review_data.train_data[0]['text'])\n",
    "\n",
    "train_tokens = prep_utils.get_set_tokens(\n",
    "    review_data.train_data[0]['text'], tokenizer, set='train', task='task1')\n",
    "\n",
    "test_tokens = prep_utils.get_set_tokens(\n",
    "    review_data.test_data[0]['text'], tokenizer, set='test', task='task1')\n",
    "\n",
    "val_tokens = prep_utils.get_set_tokens(\n",
    "    review_data.val_data[0]['text'], tokenizer, set='val', task='task1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pretrained word embedding from ./data/embedding/glove.twitter.27B.100d.txt...\n",
      "\t0 words loaded\n",
      "\t50000 words loaded\n",
      "\t100000 words loaded\n",
      "\t150000 words loaded\n",
      "\t200000 words loaded\n",
      "\t250000 words loaded\n",
      "\t300000 words loaded\n",
      "\t350000 words loaded\n",
      "\t400000 words loaded\n",
      "\t450000 words loaded\n",
      "\t500000 words loaded\n",
      "\t550000 words loaded\n",
      "\t600000 words loaded\n",
      "\t650000 words loaded\n",
      "\t700000 words loaded\n",
      "\t750000 words loaded\n",
      "\t800000 words loaded\n",
      "\t850000 words loaded\n",
      "\t900000 words loaded\n",
      "\t950000 words loaded\n",
      "\t1000000 words loaded\n",
      "\t1050000 words loaded\n",
      "\t1100000 words loaded\n",
      "\t1150000 words loaded\n",
      "Creating embedding matrix...\n",
      "\t61.0% words vector not found (169916 over 278570)\n",
      "...embedding matrix created (matrix pickled at ./data/embedding/task1_embedding_matrix.npy)\n"
     ]
    }
   ],
   "source": [
    "# create embedding matrix\n",
    "\n",
    "# get embedded matrix based containing vectors from a pretrained dict\n",
    "# vectors are related only to words found in train sentences\n",
    "e_matrix = prep_utils.get_embedding_matrix(const.word_embedding_filepath, 'task1',\n",
    "                                            tokenizer, len(tokenizer.index_word)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278569\n",
      "278570\n"
     ]
    }
   ],
   "source": [
    "#  TODO\n",
    "embedding_size = 32\n",
    "\n",
    "word_vector_dim = 100\n",
    "vocab_size = len(tokenizer.word_index) +1\n",
    "\n",
    "max_length = len(max(train_tokens, key=len))\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, word_vector_dim,\n",
    "                        embeddings_initializer=Constant(e_matrix), trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)  # the embedding layer\n",
    "model.add(LSTM(15, dropout=0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_tokens, review_data.train_data[1], epochs=5,validation_data=(val_tokens, review_data.val_data[1]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb20caab5ee8cc09ab8b75b23426de7ab409d2661b4ee85e5bcb1125eed550bf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
