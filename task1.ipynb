{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1\n",
    "Riconoscimento review positiva o negativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from libraries.dataset import Dataset\n",
    "\n",
    "import libraries.preprocessing_utils as prep_utils\n",
    "import libraries.models_builders as models\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "# import libraries.utils as utils\n",
    "import constants as const\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data retrieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./data/balanced_review_sentiment_train.csv...\n",
      "File loaded in 0.39 minutes\n",
      "Reading ./data/balanced_review_sentiment_val.csv...\n",
      "File loaded in 0.01 minutes\n",
      "Reading ./data/balanced_review_sentiment_test.csv...\n",
      "File loaded in 0.01 minutes\n"
     ]
    }
   ],
   "source": [
    "review_data = Dataset('review', 'sentiment')\n",
    "review_data.split(['text'], 'sentiment', n_samples=500_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "### Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text -> list[str]\n",
    "# fit tokenizer and tokenize\n",
    "tokenizer = prep_utils.get_tokenizer(review_data.train_data[0]['text'])\n",
    "\n",
    "train_tokens = prep_utils.get_set_tokens(\n",
    "    review_data.train_data[0]['text'], tokenizer, set='train', task='task1')\n",
    "\n",
    "test_tokens = prep_utils.get_set_tokens(\n",
    "    review_data.test_data[0]['text'], tokenizer, set='test', task='task1')\n",
    "\n",
    "val_tokens = prep_utils.get_set_tokens(\n",
    "    review_data.val_data[0]['text'], tokenizer, set='val', task='task1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled embedding matrix from ./data/embedding/task1_embedding_matrix.npy...\n",
      "...embedding matrix loaded\n"
     ]
    }
   ],
   "source": [
    "# create embedding matrix\n",
    "\n",
    "# get embedded matrix based containing vectors from a pretrained dict\n",
    "# vectors are related only to words found in train sentences\n",
    "e_matrix = prep_utils.get_embedding_matrix(const.word_embedding_filepath, 'task1',\n",
    "                                            tokenizer, len(tokenizer.index_word)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 32\n",
    "word_vector_dim = 100\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) +1\n",
    "max_length = len(max(train_tokens, key=len))\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, word_vector_dim,\n",
    "                            embeddings_initializer=Constant(e_matrix), trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the functions that return the hypermodel with a specific hyperparameters search space.\n",
    "\n",
    "Hyperparameters:\n",
    "- number of units in dense layer\n",
    "- dropout (yes/no) in order to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "Cell and Hidden states are vectors which have a specific dimension (units parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 3\n",
      "dropout (Choice)\n",
      "{'default': 0.2, 'conditions': [], 'values': [0.2, 0.5], 'ordered': True}\n",
      "units (Choice)\n",
      "{'default': 15, 'conditions': [], 'values': [15, 20, 50], 'ordered': True}\n",
      "lr (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001], 'ordered': True}\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "dropout           |0.2               |?                 \n",
      "units             |50                |?                 \n",
      "lr                |0.01              |?                 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10)\n",
    "\n",
    "def build_rnn_model(hp):\n",
    "    # Define the hyperparams\n",
    "    dropout = hp.Choice(\"dropout\", [0.2, 0.5])\n",
    "    lstm_units = hp.Choice(\"units\", [15,20,50])\n",
    "    lr = hp.Choice(\"lr\", [0.01, 0.001] )\n",
    "\n",
    "    model = Sequential()    \n",
    "    model.add(embedding_layer)  # the embedding layer\n",
    "    model.add(LSTM(lstm_units, dropout=dropout))\n",
    "    # if dropout:\n",
    "    #     model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_rnn_model,\n",
    "    objective='val_accuracy',\n",
    "    overwrite = True,\n",
    "    max_trials=2,\n",
    "    # directory=os.path.normpath(\"D:/keras_tuner\"),\n",
    "    )\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# The model-building function is called with different hyperparams values in different trial.\n",
    "# In each trial, the tuner would generate a new set of hyperparameter values.\n",
    "# The model is then fit and evaluated. The metrics are recorded.\n",
    "# The tuner progressively explores the space and finally finds a good set of hyperparams values.\n",
    "tuner.search(train_tokens[:50], review_data.train_data[1][:50], batch_size=128, epochs=1,validation_data=(val_tokens[:500], review_data.val_data[1][:500]), callbacks=[stop_early, keras.callbacks.TensorBoard(\"./logs\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 13456), started 0:10:15 ago. (Use '!kill 13456' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f57daf8cf5e05efd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f57daf8cf5e05efd\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=\"./untitled_project/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "157/157 [==============================] - 17s 72ms/step - loss: 0.6920 - accuracy: 0.5236 - val_loss: 0.6771 - val_accuracy: 0.5400\n",
      "Epoch 2/500\n",
      "157/157 [==============================] - 10s 64ms/step - loss: 0.6677 - accuracy: 0.5908 - val_loss: 0.6375 - val_accuracy: 0.6420\n",
      "Epoch 3/500\n",
      "157/157 [==============================] - 10s 64ms/step - loss: 0.6335 - accuracy: 0.6428 - val_loss: 0.6566 - val_accuracy: 0.6260\n",
      "Epoch 4/500\n",
      "157/157 [==============================] - 10s 64ms/step - loss: 0.6030 - accuracy: 0.6730 - val_loss: 0.6462 - val_accuracy: 0.6220\n",
      "Epoch 5/500\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.5847 - accuracy: 0.6876 - val_loss: 0.5818 - val_accuracy: 0.6940\n",
      "Epoch 6/500\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.5594 - accuracy: 0.7130 - val_loss: 0.6047 - val_accuracy: 0.7140\n",
      "Epoch 7/500\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.5439 - accuracy: 0.7312 - val_loss: 0.6781 - val_accuracy: 0.6460\n",
      "Epoch 8/500\n",
      "157/157 [==============================] - 9s 55ms/step - loss: 0.5229 - accuracy: 0.7440 - val_loss: 0.5412 - val_accuracy: 0.7420\n",
      "Epoch 9/500\n",
      "157/157 [==============================] - 8s 53ms/step - loss: 0.5146 - accuracy: 0.7488 - val_loss: 0.5284 - val_accuracy: 0.7620\n",
      "Epoch 10/500\n",
      "157/157 [==============================] - 8s 54ms/step - loss: 0.4920 - accuracy: 0.7586 - val_loss: 0.7049 - val_accuracy: 0.6420\n",
      "Epoch 11/500\n",
      "157/157 [==============================] - 8s 53ms/step - loss: 0.4817 - accuracy: 0.7694 - val_loss: 0.6473 - val_accuracy: 0.6720\n",
      "Epoch 12/500\n",
      "157/157 [==============================] - 9s 54ms/step - loss: 0.4709 - accuracy: 0.7798 - val_loss: 0.5092 - val_accuracy: 0.7640\n",
      "Epoch 13/500\n",
      "157/157 [==============================] - 9s 59ms/step - loss: 0.4631 - accuracy: 0.7784 - val_loss: 0.5185 - val_accuracy: 0.7540\n",
      "Epoch 14/500\n",
      "157/157 [==============================] - 9s 59ms/step - loss: 0.4481 - accuracy: 0.7934 - val_loss: 0.6436 - val_accuracy: 0.7120\n",
      "Epoch 15/500\n",
      "157/157 [==============================] - 9s 59ms/step - loss: 0.4319 - accuracy: 0.7976 - val_loss: 0.5440 - val_accuracy: 0.7540\n",
      "Epoch 16/500\n",
      "157/157 [==============================] - 9s 60ms/step - loss: 0.4314 - accuracy: 0.8032 - val_loss: 0.5533 - val_accuracy: 0.7400\n",
      "Epoch 17/500\n",
      "157/157 [==============================] - 9s 61ms/step - loss: 0.4157 - accuracy: 0.8160 - val_loss: 0.4982 - val_accuracy: 0.7700\n",
      "Epoch 18/500\n",
      "157/157 [==============================] - 9s 55ms/step - loss: 0.4097 - accuracy: 0.8132 - val_loss: 0.5726 - val_accuracy: 0.7520\n",
      "Epoch 19/500\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.3987 - accuracy: 0.8184 - val_loss: 0.5037 - val_accuracy: 0.7720\n",
      "Epoch 20/500\n",
      "157/157 [==============================] - 9s 60ms/step - loss: 0.3895 - accuracy: 0.8280 - val_loss: 0.5493 - val_accuracy: 0.7660\n",
      "Epoch 21/500\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.3808 - accuracy: 0.8338 - val_loss: 0.7536 - val_accuracy: 0.7000\n",
      "Epoch 22/500\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.3761 - accuracy: 0.8324 - val_loss: 0.6680 - val_accuracy: 0.7060\n",
      "Epoch 23/500\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.3560 - accuracy: 0.8438 - val_loss: 0.6263 - val_accuracy: 0.7480\n",
      "Epoch 24/500\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.3605 - accuracy: 0.8400 - val_loss: 0.5146 - val_accuracy: 0.7820\n",
      "Epoch 25/500\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.3521 - accuracy: 0.8528 - val_loss: 0.5472 - val_accuracy: 0.7760\n",
      "Epoch 26/500\n",
      "157/157 [==============================] - 11s 67ms/step - loss: 0.3452 - accuracy: 0.8480 - val_loss: 0.6275 - val_accuracy: 0.7420\n",
      "Epoch 27/500\n",
      "157/157 [==============================] - 9s 60ms/step - loss: 0.3482 - accuracy: 0.8486 - val_loss: 0.5174 - val_accuracy: 0.7740\n",
      "Epoch 28/500\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.3353 - accuracy: 0.8540 - val_loss: 0.5311 - val_accuracy: 0.7800\n",
      "Epoch 29/500\n",
      "157/157 [==============================] - 9s 59ms/step - loss: 0.3288 - accuracy: 0.8580 - val_loss: 0.5065 - val_accuracy: 0.7780\n",
      "Epoch 30/500\n",
      "157/157 [==============================] - 9s 60ms/step - loss: 0.3230 - accuracy: 0.8590 - val_loss: 0.5819 - val_accuracy: 0.7580\n",
      "Epoch 31/500\n",
      "157/157 [==============================] - 9s 56ms/step - loss: 0.3158 - accuracy: 0.8662 - val_loss: 0.5691 - val_accuracy: 0.7680\n",
      "Epoch 32/500\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.3055 - accuracy: 0.8742 - val_loss: 0.5776 - val_accuracy: 0.7620\n",
      "Epoch 33/500\n",
      "157/157 [==============================] - 8s 51ms/step - loss: 0.3098 - accuracy: 0.8646 - val_loss: 0.6104 - val_accuracy: 0.7480\n",
      "Epoch 34/500\n",
      "157/157 [==============================] - 8s 52ms/step - loss: 0.2877 - accuracy: 0.8772 - val_loss: 0.5065 - val_accuracy: 0.7640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x192fb734640>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the optimal hyperparameters from the results\n",
    "best_hps=tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10)\n",
    "\n",
    "# Build model\n",
    "h_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the hypertuned model\n",
    "h_model.fit(train_tokens[:5000], review_data.train_data[1][:5000], epochs=500, validation_data=(val_tokens[:2000], review_data.val_data[1][:2000]), callbacks=[stop_early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.rho\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         27857000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 20)                9680      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 27,866,701\n",
      "Trainable params: 9,701\n",
      "Non-trainable params: 27,857,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model = tuner.get_best_models()[0]\n",
    "\n",
    "# tuner.results_summary()\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         27857000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 15)                6960      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 27,863,976\n",
      "Trainable params: 6,976\n",
      "Non-trainable params: 27,857,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  TODO\n",
    "embedding_size = 32\n",
    "batch_size = 128\n",
    "\n",
    "word_vector_dim = 100\n",
    "vocab_size = len(tokenizer.word_index) +1\n",
    "\n",
    "max_length = len(max(train_tokens, key=len))\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, word_vector_dim,\n",
    "                        embeddings_initializer=Constant(e_matrix), trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)  # the embedding layer\n",
    "model.add(LSTM(15, dropout=0.5))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7657/7657 [==============================] - 798s 104ms/step - loss: 0.5299 - accuracy: 0.7327 - val_loss: 0.4053 - val_accuracy: 0.8209\n",
      "Epoch 2/5\n",
      "2412/7657 [========>.....................] - ETA: 9:08 - loss: 0.4742 - accuracy: 0.7753"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_tokens, review_data.train_data[1], batch_size=batch_size, epochs=5,validation_data=(val_tokens, review_data.val_data[1]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb20caab5ee8cc09ab8b75b23426de7ab409d2661b4ee85e5bcb1125eed550bf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
