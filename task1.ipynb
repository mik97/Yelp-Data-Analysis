{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1\n",
    "Riconoscimento review positiva o negativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\aless_vzq3wiu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aless_vzq3wiu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aless_vzq3wiu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from libraries.dataset import Dataset\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import libraries.preprocessing_utils as prep_utils\n",
    "import libraries.models_builders as models_builders\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "import constants as const"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data retrieving and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./data_100_000/balanced_review_sentiment_train.csv...\n",
      "File loaded in 0.0391 minutes\n",
      "Reading ./data_100_000/balanced_review_sentiment_val.csv...\n",
      "File loaded in 0.0006 minutes\n",
      "Reading ./data_100_000/balanced_review_sentiment_test.csv...\n",
      "File loaded in 0.0005 minutes\n"
     ]
    }
   ],
   "source": [
    "review_data = Dataset('review', 'sentiment')\n",
    "#  50_000 elements for each class\n",
    "review_data.split(['text'], 'sentiment', n_samples=50_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text -> list[str]\n",
    "# fit tokenizer and tokenize\n",
    "tokenizer = prep_utils.get_tokenizer(review_data.train_data[0]['text'])\n",
    "\n",
    "train_tokens = prep_utils.get_set_tokens(\n",
    "    review_data.train_data[0]['text'], tokenizer, set='train', task='task1')\n",
    "\n",
    "test_tokens = prep_utils.get_set_tokens(\n",
    "    review_data.test_data[0]['text'], tokenizer, set='test', task='task1')\n",
    "\n",
    "val_tokens = prep_utils.get_set_tokens(\n",
    "    review_data.val_data[0]['text'], tokenizer, set='val', task='task1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use another texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled cleaned sentences data from ./data_100_000/pickled/task1_train_cleaned_sentences.pkl...\n",
      "Loading pickled cleaned sentences data from ./data_100_000/pickled/task1_test_cleaned_sentences.pkl...\n",
      "Loading pickled cleaned sentences data from ./data_100_000/pickled/task1_val_cleaned_sentences.pkl...\n"
     ]
    }
   ],
   "source": [
    "import libraries.filenames_generator as filenames  \n",
    "# \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "prep_train_data = prep_utils.preprocess_texts(review_data.train_data[0]['text'], path= filenames.picked_cleaned_sentences(\n",
    "        \"train\", \"task1\"))\n",
    "\n",
    "prep_test_data = prep_utils.preprocess_texts(review_data.test_data[0]['text'], path= filenames.picked_cleaned_sentences(\n",
    "        \"test\", \"task1\"))\n",
    "\n",
    "prep_val_data = prep_utils.preprocess_texts(review_data.val_data[0]['text'], path= filenames.picked_cleaned_sentences(\n",
    "        \"val\", \"task1\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "nb_train_data = vectorizer.fit_transform(prep_train_data[:30_000]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(nb_train_data, review_data.train_data[1][:30_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test_data = vectorizer.transform(prep_test_data).toarray()\n",
    "\n",
    "nb_model.score(nb_test_data, review_data.test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "res = nb_model.predict(nb_test_data[i].reshape(1, -1))\n",
    "\n",
    "print(f'''\n",
    "REVIEW:\n",
    "{review_data.test_data[0]['text'][i]}\n",
    "\n",
    "REAL SENTIMENT: {review_data.test_data[1][i]} \n",
    "PREDICTED SENTIMENT: {res} - {'positive' if res else 'negative'}''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_matrix = prep_utils.get_embedding_matrix(const.word_embedding_filepath, 'task1',\n",
    "                                            tokenizer, len(tokenizer.index_word)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_dim = 100\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) +1\n",
    "max_length = len(max(train_tokens, key=len))\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, word_vector_dim,\n",
    "                            embeddings_initializer=Constant(e_matrix), trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the functions that return the hypermodel with a specific hyperparameters search space.\n",
    "\n",
    "Hyperparameters:\n",
    "- number of units\n",
    "- dropout (yes/no) in order to prevent overfitting\n",
    "- learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "Cell and Hidden states are vectors which have a specific dimension (units parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom callbacks\n",
    "stop_early_cb = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First hyperparams trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"task1_lstm_adam_128\"\n",
    "\n",
    "builder = models_builders.get_rnn_builder(\n",
    "    drop=[0.2, 0.5],\n",
    "    units=[15, 20, 50, 80],\n",
    "    lrate=[0.01, 0.001],\n",
    "    optimizer=keras.optimizers.Adam,\n",
    "    embedding_layer=embedding_layer)\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    builder,\n",
    "    objective = 'val_accuracy',\n",
    "    max_trials = 10,\n",
    "    directory = const.tuner_path, project_name = project_name\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_tokens, review_data.train_data[1],\n",
    "             batch_size=128, epochs=1000,\n",
    "             validation_data=(val_tokens, review_data.val_data[1]),\n",
    "             callbacks=[\n",
    "                 stop_early_cb,\n",
    "                 tf.keras.callbacks.TensorBoard(const.logs_path + project_name, update_freq='epoch')],\n",
    "             verbose=0)\n",
    "\n",
    "#  executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second hyperparameters trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"task1_lstm_adam_64_new\"\n",
    "\n",
    "builder = models_builders.get_rnn_builder(\n",
    "    drop=[0.2, 0.5],\n",
    "    units=[100, 150],\n",
    "    lrate=[0.1, 0.01, 0.001],\n",
    "    optimizer=keras.optimizers.Adam,\n",
    "    embedding_layer=embedding_layer)\n",
    "\n",
    "tuner1 = kt.RandomSearch(\n",
    "    builder,\n",
    "    objective = 'val_accuracy',\n",
    "    max_trials = 10,\n",
    "    directory = const.tuner_path, project_name = \"task1_lstm_adam_64\"\n",
    ")\n",
    "\n",
    "tuner1.search_space_summary()\n",
    "\n",
    "# executed\n",
    "tuner1.search(train_tokens, review_data.train_data[1],\n",
    "             batch_size=64, epochs=1000,\n",
    "             validation_data=(val_tokens, review_data.val_data[1]),\n",
    "             callbacks=[\n",
    "                 stop_early_cb,\n",
    "                 tf.keras.callbacks.TensorBoard(const.logs_path + project_name, update_freq='epoch')],\n",
    "             verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the optimal hyperparameters from the results\n",
    "# best_hps=tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# # Build model\n",
    "# h_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# # Train the hypertuned model\n",
    "# h_model.fit(train_tokens, review_data.train_data[1], epochs=1000, validation_data=(val_tokens, review_data.val_data[1]), callbacks=[stop_early_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = tuner.get_best_models()[0]\n",
    "\n",
    "# # tuner.results_summary()\n",
    "# best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load models from tf hub, small Bert chosen\n",
    "handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
    "handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTA: hub.kerasLayer -> wrappa un SavedModel (scaricato dall'hub) in un keras layer\n",
    "\n",
    "def build_classifier_model():\n",
    "  # crea un tensore simbolico rappresentante l'input, necessario per la\n",
    "  # costruzione iniziale del modello keras  \n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "\n",
    "  # -- creazione preprocessing layer e preprocessing dei dati -- \n",
    "  preprocessing_layer = hub.KerasLayer(handle_preprocess, name='preprocessing')\n",
    "  # frasi processate dal preprocessing che saranno inputs dell'encoder \n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "\n",
    "  # -- creazione enconder layer e generazione output --\n",
    "  encoder = hub.KerasLayer(handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  \n",
    "  # -- def net --\n",
    "  # dense-> dropout -> output\n",
    "  net = outputs['pooled_output'] # prendiamo in considerazione solo questo output\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  prepariamo i dati\n",
    "train_df = tf.data.Dataset.from_tensor_slices((prep_train_data[:1000], review_data.train_data[1][:1000]))\n",
    "val_df = tf.data.Dataset.from_tensor_slices((prep_val_data[:10], review_data.val_data[1][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_df).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps, # lr decay\n",
    "                                          optimizer_type='adamw')\n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'yellow dog eats cool funky cafe lot crazy funky food went first time visiting friend orlando area past november looking casual place grab lunch couple beer came radar say yelp app menu large offer lot sandwich option many slathered lot delicious unhealthy thing almost went lighter dish thought hell vacation got fig sandwich pulled pork goat cheese bacon fried onion house made jalapeno fig sauce let tell thing fricken yuge massive piled pulled pork super messy sticky bc cheese sauce could barely eat yet somehow managed girl mess around come good food nice selection draft beer wash yummy sammies including one local fave reef donkey tampa bay brewing friend couple yuenglings definitely place would go diet pretty awesome atmosphere part hippy part hipster floridian sweet outdoor patio imagine size two bar likely get pretty busy weekend food counter service go place order call name pick counter service pretty friendly would definitely go back area', shape=(), dtype=string)\n",
      "tf.Tensor(1.0, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_df.take(1):  # only take first element of dataset\n",
    "    print(images)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "                loss=loss,\n",
    "                metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "32/32 [==============================] - 300s 9s/step - loss: 0.8201 - binary_accuracy: 0.5150 - val_loss: 0.7166 - val_binary_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      " 7/32 [=====>........................] - ETA: 3:41 - loss: 0.7139 - binary_accuracy: 0.5402"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Alessandra Boccuto\\Documenti\\University\\UNIBO\\Materie\\Intelligenza artificiale\\progetto\\Yelp-Data-Analysis\\task1.ipynb Cell 36'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Alessandra%20Boccuto/Documenti/University/UNIBO/Materie/Intelligenza%20artificiale/progetto/Yelp-Data-Analysis/task1.ipynb#ch0000035?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mtrain_df\u001b[39m.\u001b[39;49mbatch(\u001b[39m32\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Alessandra%20Boccuto/Documenti/University/UNIBO/Materie/Intelligenza%20artificiale/progetto/Yelp-Data-Analysis/task1.ipynb#ch0000035?line=1'>2</a>\u001b[0m                     validation_data\u001b[39m=\u001b[39;49mval_df\u001b[39m.\u001b[39;49mbatch(\u001b[39m1\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Alessandra%20Boccuto/Documenti/University/UNIBO/Materie/Intelligenza%20artificiale/progetto/Yelp-Data-Analysis/task1.ipynb#ch0000035?line=2'>3</a>\u001b[0m                     epochs\u001b[39m=\u001b[39;49mepochs)\n",
      "File \u001b[1;32md:\\Programmi\\venvs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Programmi\\venvs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/keras/engine/training.py?line=1376'>1377</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/keras/engine/training.py?line=1377'>1378</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/keras/engine/training.py?line=1378'>1379</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/keras/engine/training.py?line=1379'>1380</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/keras/engine/training.py?line=1380'>1381</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/keras/engine/training.py?line=1381'>1382</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/keras/engine/training.py?line=1382'>1383</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/keras/engine/training.py?line=1383'>1384</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/keras/engine/training.py?line=1384'>1385</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/keras/engine/training.py?line=1385'>1386</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\Programmi\\venvs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Programmi\\venvs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\Programmi\\venvs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\Programmi\\venvs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=2952'>2953</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=2953'>2954</a>\u001b[0m   (graph_function,\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=2954'>2955</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=2955'>2956</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=2956'>2957</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32md:\\Programmi\\venvs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=1848'>1849</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=1849'>1850</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=1850'>1851</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=1851'>1852</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=1852'>1853</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=1853'>1854</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=1854'>1855</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m     args,\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m     possible_gradient_type,\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     executing_eagerly)\n\u001b[0;32m   <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\Programmi\\venvs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=506'>507</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=507'>508</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=510'>511</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/function.py?line=511'>512</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32md:\\Programmi\\venvs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='file:///d%3A/Programmi/venvs/tf-gpu/lib/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x=train_df.batch(32),\n",
    "                    validation_data=val_df.batch(1),\n",
    "                    epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb20caab5ee8cc09ab8b75b23426de7ab409d2661b4ee85e5bcb1125eed550bf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
